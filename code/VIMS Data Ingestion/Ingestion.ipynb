{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import urllib.request\n",
    "from pprint import pprint\n",
    "from html_table_parser.parser import HTMLTableParser\n",
    "from urllib.parse import urljoin\n",
    "import csv\n",
    "# for converting the parsed data in a\n",
    "# pandas dataframe\n",
    "import pandas as pd\n",
    "baseurl = 'https://vims.univ-nantes.fr'\n",
    "searchurl = 'https://vims.univ-nantes.fr/target/titan'\n",
    "csv_save_path = 'nantes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_directory_for_links(search_directory):\n",
    "    reqs = requests.get(searchurl)\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "    abc = soup.find_all('a')\n",
    "    allurls = [urljoin(search_directory,link.get('href')) for link in abc]\n",
    "    # url filtering\n",
    "    urls = []\n",
    "    for link in allurls:\n",
    "        if \"/flyby\" in link:\n",
    "            urls.append(link)\n",
    "    if urls:\n",
    "        print(\"found flybys\")\n",
    "    else:\n",
    "        print(\"no flybys found\")\n",
    "    return urls\n",
    "\n",
    "\n",
    "urls = parse_directory_for_links(searchurl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_get_contents(url):\n",
    "    req = urllib.request.Request(url=url)\n",
    "    f = urllib.request.urlopen(req)\n",
    "    # reading contents of the website\n",
    "    return f.read()\n",
    "\n",
    "def list_all_tables(urllist):\n",
    "    p = HTMLTableParser()\n",
    "    start = time.time()\n",
    "    length = len(urllist)\n",
    "    for index,url in enumerate(urllist):\n",
    "        html = url_get_contents(url).decode('utf-8')\n",
    "        p.feed(html)\n",
    "        end = time.time() - start\n",
    "        print(url, \"                                                                                            \")\n",
    "        print(url, \"passed time\",str(end) + \"/\" +str(length/(index+1)*end) + \" seconds             \", end = \"\\r\")\n",
    "    return [p.tables], np.array(p.tables)\n",
    "flybylist, flybyarray = list_all_tables(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_table_with_url(flybyarray, urls):\n",
    "    return {url :flybyarray[index] for index, url in enumerate(urls)} \n",
    "flybyarrays = combine_table_with_url(flybyarray, urls)\n",
    "print(flybyarray)\n",
    "indexOfFlybyIndex = 2\n",
    "infexOfFlybyDate = 0\n",
    "indexOfCubeQuantity = 3\n",
    "indexOfAltitude = 4\n",
    "indexOfCassiniMission = 5\n",
    "indexOfSequence = 6\n",
    "indexOfRevolution = 7\n",
    "\n",
    "with open(csv_save_path, 'w', newline = \"\") as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter = \",\")\n",
    "    writer.writerow([\"flyby_url\" ,\"flyby_index\",\"flyby_cassini_mission\",\"flyby_date\",\"flyby_altitude\",\"flyby_cube_quantity\",\"flyby_sequence\",\"flyby_revolution\"])\n",
    "    for index, flyby in enumerate(flybyarray):\n",
    "        writer.writerow([urls[index], flyby[indexOfFlybyIndex][1], flyby[indexOfCassiniMission][1], flyby[infexOfFlybyDate][1],flyby[indexOfAltitude][1],flyby[indexOfCubeQuantity][1],flyby[indexOfSequence][1],flyby[indexOfRevolution][1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
